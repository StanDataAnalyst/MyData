{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d5ead1f-9900-407b-8f64-756164571f38",
   "metadata": {},
   "source": [
    "## ETL Automation with MySQL: Dynamic Data Integration and Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75e1c5c-045f-4b90-a7cf-18cdf6cf5694",
   "metadata": {},
   "source": [
    "This Python script automates the ETL (Extract, Transform, Load) process by seamlessly integrating data between two MySQL databases. The script utilizes robust libraries such as sqlalchemy for database operations, pandas for data manipulation, and logging for comprehensive monitoring of the process. It begins by extracting data from source tables into DataFrames, followed by performing an UPSERT operation to update or insert records in the target database.\n",
    "\n",
    "The UPSERT functionality, powered by SQLAlchemy's dialect-specific capabilities, ensures data consistency and eliminates duplicate records. Additionally, the script incorporates best practices for memory management by programmatically deleting unused DataFrames and invoking garbage collection to optimize performance. Detailed logging at every step ensures transparency, facilitates debugging, and provides a clear audit trail of operations. This implementation highlights the efficiency of Python in handling large-scale data workflows in a secure and scalable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33cb28b2-b08e-4c85-85b6-e83753d26074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging      # For log messages\n",
    "import os           # To interact with the operating system\n",
    "from sqlalchemy import create_engine, Table, MetaData  # For database operations\n",
    "import pandas as pd         # For data manipulation\n",
    "from sqlalchemy.dialects.mysql import insert        # Import the insert function for creating SQL statements\n",
    "import traceback        # Print and format exceptions\n",
    "import gc           # To free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52478c32-772c-4c8e-81ad-c09dc98a0884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the log directory exists, otherwise create\n",
    "log_dir = 'C:/log/python/etl'\n",
    "log_file = os.path.join(log_dir, 'etl_log.log')\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=log_file, level=logging.DEBUG,  # To capture all levels of log messages\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s') # Include timestamp, log level, and message\n",
    "\n",
    "# Example log messages\n",
    "logging.debug('This is a DEBUG message.')\n",
    "logging.info('This is an INFO message.')\n",
    "logging.warning('This is a WARNING message.')\n",
    "logging.error('This is an ERROR message.')\n",
    "logging.critical('This is a CRITICAL message.')\n",
    "\n",
    "logging.info('Starting the script.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccdac87-aacd-45b1-878d-b3b8c4d8d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source MySQL connection details\n",
    "source_db_url = 'mysql+pymysql://etluser:eXtract2215$@192.168.1.101:3306/sakila'\n",
    "\n",
    "# Target MySQL connection details to target database\n",
    "target_db_url = 'mysql+pymysql://etluser:eXtract2215$@192.168.1.200:3306/synthea'\n",
    "\n",
    "# Create engine instances for both source and target databases\n",
    "source_engine = create_engine(source_db_url)\n",
    "target_engine = create_engine(target_db_url)\n",
    "logging.info('Database engines created for both source and target databases.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32853570-0510-4c64-b86b-fe3987d617c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform UPSERT\n",
    "def upsert(df, table_name, conn):\n",
    "    meta = MetaData()       # Create a MetaData instance to hold table schema info\n",
    "    table = Table(table_name, meta, autoload_with=conn)     # Load the table schema from a database\n",
    "    \n",
    "    # Convert DataFrame to list of dictionaries\n",
    "    data = df.to_dict(orient='records')\n",
    "    \n",
    "    # Build the insert statement with on duplicate key update\n",
    "    stmt = insert(table).values(data)\n",
    "    update_dict = {c.name: c for c in stmt.inserted}    # Prepare a dictionary for updating rows on conflict\n",
    "    stmt = stmt.on_duplicate_key_update(update_dict)    # Adds the ON DUPLICATE KEY UPDATE clause\n",
    "    \n",
    "    # Logs the compiled SQL statement for debugging\n",
    "    logging.debug(stmt.compile(dialect=conn.dialect))\n",
    "    \n",
    "    try:\n",
    "        # Execute the statement\n",
    "        result = conn.execute(stmt)\n",
    "        conn.commit()  # Explicitly commit the transaction\n",
    "        logging.info(f\"{result.rowcount} rows affected in {table_name}.\")   # Logs the number of affected rows\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error occurred:\", exc_info=True)     # Logs any errors with a traceback\n",
    "        conn.rollback()  # Rollback if error occurs\n",
    "\n",
    "try:\n",
    "    with source_engine.connect() as src_conn, target_engine.connect() as tgt_conn:  # Establish connections to both databases\n",
    "        # Load data into DataFrames from the source database\n",
    "        df_actor = pd.read_sql(\"SELECT * FROM actor\", con=src_conn)\n",
    "        df_customer = pd.read_sql(\"SELECT * FROM customer\", con=src_conn)\n",
    "        logging.info(\"Data loaded into DataFrames from the source database.\")\n",
    "\n",
    "        # Perform UPSERT operation into each table in the target database\n",
    "        upsert(df_actor, 'actor_01', tgt_conn)\n",
    "        upsert(df_customer, 'customer_01', tgt_conn)\n",
    "\n",
    "finally:\n",
    "    source_engine.dispose()    # Disposes of the source database engine\n",
    "    target_engine.dispose()    # Disposes of the target database engine\n",
    "    logging.info(\"Database connections closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a468245-dba3-444f-9f86-db4bd8b6ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to delete all DataFrames\n",
    "def delete_all_dataframes():\n",
    "    # Get a list of all variable names in the global namespace\n",
    "    all_variables = list(globals().keys())\n",
    "\n",
    "    # Iterate over all variable names\n",
    "    for var_name in all_variables:\n",
    "        # Check if the variable is a DataFrame\n",
    "        if isinstance(globals()[var_name], pd.DataFrame):       # Check if the variable is a DataFrame\n",
    "            del globals()[var_name]     # Delete the DataFrame\n",
    "            logging.info(f\"Deleted DataFrame: {var_name}\")  # Log the deletion of the DataFrame\n",
    "\n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    logging.info(\"Garbage collection completed.\")\n",
    "\n",
    "# Delete all DataFrames\n",
    "delete_all_dataframes()\n",
    "\n",
    "# Check if DataFrames have been deleted\n",
    "try:\n",
    "    print(df_actor.head())\n",
    "except NameError:\n",
    "    logging.info(\"df_actor has been deleted\")\n",
    "\n",
    "try:\n",
    "    print(df_customer.head())\n",
    "except NameError:\n",
    "    logging.info(\"df_customer has been deleted\")\n",
    "\n",
    "# End of script\n",
    "logging.info(\"Script completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
